{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SFNet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.user_pathway = config[\"user_pathway\"][:]\n",
    "        self.item_pathway = config[\"item_pathway\"][:]\n",
    "        self.combined_pathway = config[\"combined_pathway\"][:]\n",
    "        self.embedding_dim = config[\"embedding_dim\"]\n",
    "\n",
    "        self.user_embedding = nn.Embedding(\n",
    "            num_embeddings=config[\"num_user_emb\"],\n",
    "            embedding_dim=self.embedding_dim,\n",
    "            max_norm=1.0,\n",
    "        )\n",
    "        self.cup_size_embedding = nn.Embedding(\n",
    "            num_embeddings=config[\"num_cup_size_emb\"],\n",
    "            embedding_dim=self.embedding_dim,\n",
    "            max_norm=1.0,\n",
    "        )\n",
    "        self.item_embedding = nn.Embedding(\n",
    "            num_embeddings=config[\"num_item_emb\"],\n",
    "            embedding_dim=self.embedding_dim,\n",
    "            max_norm=1.0,\n",
    "        )\n",
    "        self.category_embedding = nn.Embedding(\n",
    "            num_embeddings=config[\"num_category_emb\"],\n",
    "            embedding_dim=self.embedding_dim,\n",
    "            max_norm=1.0,\n",
    "        )\n",
    "\n",
    "        # Customer pathway transformation\n",
    "        # user_embedding_dim + cup_size_embedding_dim + num_user_numeric_features\n",
    "        user_features_input_size = 2 * self.embedding_dim + config[\"num_user_numeric\"]\n",
    "        self.user_pathway.insert(0, user_features_input_size)\n",
    "        self.user_transform_blocks = []\n",
    "        for i in range(1, len(self.user_pathway)):\n",
    "            self.user_transform_blocks.append(\n",
    "                SkipBlock(\n",
    "                    self.user_pathway[i - 1],\n",
    "                    self.user_pathway[i],\n",
    "                    config[\"activation\"],\n",
    "                )\n",
    "            )\n",
    "            self.user_transform_blocks.append(nn.Dropout(p=config[\"dropout\"]))\n",
    "        self.user_transform_blocks = nn.Sequential(*self.user_transform_blocks)\n",
    "\n",
    "        # Article pathway transformation\n",
    "        # item_embedding_dim + category_embedding_dim + num_item_numeric_features\n",
    "        item_features_input_size = 2 * self.embedding_dim + config[\"num_item_numeric\"]\n",
    "        self.item_pathway.insert(0, item_features_input_size)\n",
    "        self.item_transform_blocks = []\n",
    "        for i in range(1, len(self.item_pathway)):\n",
    "            self.item_transform_blocks.append(\n",
    "                SkipBlock(\n",
    "                    self.item_pathway[i - 1],\n",
    "                    self.item_pathway[i],\n",
    "                    config[\"activation\"],\n",
    "                )\n",
    "            )\n",
    "            self.item_transform_blocks.append(nn.Dropout(p=config[\"dropout\"]))\n",
    "        self.item_transform_blocks = nn.Sequential(*self.item_transform_blocks)\n",
    "\n",
    "        # Combined top layer pathway\n",
    "        # u = output dim of user_transform_blocks\n",
    "        # t = output dim of item_transform_blocks\n",
    "        # Pathway combination through [u, t, |u-t|, u*t]\n",
    "        # Hence, input dimension will be 4*dim(u)\n",
    "        combined_layer_input_size = 4 * self.user_pathway[-1]\n",
    "        self.combined_pathway.insert(0, combined_layer_input_size)\n",
    "        self.combined_blocks = []\n",
    "        for i in range(1, len(self.combined_pathway)):\n",
    "            self.combined_blocks.append(\n",
    "                SkipBlock(\n",
    "                    self.combined_pathway[i - 1],\n",
    "                    self.combined_pathway[i],\n",
    "                    config[\"activation\"],\n",
    "                )\n",
    "            )\n",
    "            self.combined_blocks.append(nn.Dropout(p=config[\"dropout\"]))\n",
    "        self.combined_blocks = nn.Sequential(*self.combined_blocks)\n",
    "\n",
    "        # Linear transformation from last hidden layer to output\n",
    "        self.hidden2output = nn.Linear(self.combined_pathway[-1], config[\"num_targets\"])\n",
    "\n",
    "    def forward(self, batch_input):\n",
    "\n",
    "        # Customer Pathway\n",
    "        user_emb = self.user_embedding(batch_input[\"user_id\"])\n",
    "        cup_size_emb = self.cup_size_embedding(batch_input[\"cup_size\"])\n",
    "        user_representation = torch.cat(\n",
    "            [user_emb, cup_size_emb, batch_input[\"user_numeric\"]], dim=-1\n",
    "        )\n",
    "        user_representation = self.user_transform_blocks(user_representation)\n",
    "\n",
    "        # Article Pathway\n",
    "        item_emb = self.item_embedding(batch_input[\"item_id\"])\n",
    "        category_emb = self.category_embedding(batch_input[\"category\"])\n",
    "        item_representation = torch.cat(\n",
    "            [item_emb, category_emb, batch_input[\"item_numeric\"]], dim=-1\n",
    "        )\n",
    "        item_representation = self.item_transform_blocks(item_representation)\n",
    "\n",
    "        # Combine the pathways\n",
    "        combined_representation = self.merge_representations(\n",
    "            user_representation, item_representation\n",
    "        )\n",
    "        combined_representation = self.combined_blocks(combined_representation)\n",
    "\n",
    "        # Output layer of logits\n",
    "        logits = self.hidden2output(combined_representation)\n",
    "        pred_probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        return logits, pred_probs\n",
    "\n",
    "    def merge_representations(self, u, v):\n",
    "        \"\"\"\n",
    "        Combining two different representations via:\n",
    "        - concatenation of the two representations\n",
    "        - element-wise product u âˆ— v\n",
    "        - absolute element-wise difference |u-v|\n",
    "        Link: https://arxiv.org/pdf/1705.02364.pdf\n",
    "        \"\"\"\n",
    "        return torch.cat([u, v, torch.abs(u - v), u * v], dim=-1)\n",
    "\n",
    "\n",
    "class SkipBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, activation):\n",
    "        \"\"\"\n",
    "        Skip Connection for feed-forward block based on ResNet idea:\n",
    "        Refer: \n",
    "        - Youtube: https://www.youtube.com/watch?v=ZILIbUvp5lk\n",
    "        - Medium: https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cf51669e1624 \n",
    "                 Residual block function when the input and output dimensions are not same.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert activation in [\n",
    "            \"relu\",\n",
    "            \"tanh\",\n",
    "        ], \"Please specify a valid activation funciton: relu or tanh\"\n",
    "        if activation == \"relu\":\n",
    "            self.activation = F.relu\n",
    "        elif activation == \"tanh\":\n",
    "            self.activation = F.tanh\n",
    "\n",
    "        self.inp_transform = nn.Linear(input_dim, output_dim)\n",
    "        self.out_transform = nn.Linear(output_dim, output_dim)\n",
    "        self.inp_projection = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        y = x --> T1(x) --> ReLU(T1(x))\n",
    "        z = ReLU(T2(y) + Projection(x))\n",
    "        \"\"\"\n",
    "        y = self.activation(self.inp_transform(x))\n",
    "        z = self.activation(self.out_transform(y) + self.inp_projection(x))\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "import _jsonnet\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def load_config_from_json(config_file: str) -> Dict:\n",
    "    # load configuration\n",
    "    if not os.path.isfile(config_file):\n",
    "        raise ValueError(\"given configuration file doesn't exist\")\n",
    "    with open(config_file, \"r\") as fio:\n",
    "        config = fio.read()\n",
    "        config = json.loads(_jsonnet.evaluate_snippet(\"\", config))\n",
    "    return config\n",
    "\n",
    "\n",
    "def to_var(x, volatile=False):\n",
    "    # To convert tensors to CUDA tensors if GPU is available\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x, volatile=volatile)\n",
    "\n",
    "\n",
    "def compute_metrics(target, pred_probs):\n",
    "    \"\"\"\n",
    "    Computes metrics to report\n",
    "    \"\"\"\n",
    "    pred_labels = pred_probs.argmax(-1)\n",
    "    precision = metrics.precision_score(target, pred_labels, average=\"macro\")\n",
    "    recall = metrics.recall_score(target, pred_labels, average=\"macro\")\n",
    "    f1_score = metrics.f1_score(target, pred_labels, average=\"macro\")\n",
    "    accuracy = metrics.accuracy_score(target, pred_labels)\n",
    "    auc = metrics.roc_auc_score(target, pred_probs, average=\"macro\", multi_class=\"ovr\")\n",
    "\n",
    "    return precision, recall, f1_score, accuracy, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = \"../runs/trial_2020-Jul-04-10-16-59\"\n",
    "model_config = load_config_from_json(\n",
    "    os.path.join(saved_model_path, \"config.jsonl\")\n",
    ")\n",
    "model = SFNet(model_config[\"sfnet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.2813063 ,  0.02400028,  0.15010627,  0.11314449,  0.18134221,\n",
       "       -0.6971236 ,  0.43736464,  0.2947118 , -0.24761261, -0.16320565],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"../data/modcloth_final_data_processed_train.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_ids = df[\"item_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_embeddings = model.item_embedding(torch.tensor(item_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()\n",
    "writer.add_embedding(item_embeddings, metadata=item_ids.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
